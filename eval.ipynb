{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation (N by N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "order = 3\n",
    "\n",
    "# Randomly sample two numbers\n",
    "multiplicand = random.randint(10**(order-1), 10**order)  # Adjust the range as needed\n",
    "multiplier = random.randint(10**(order-1), 10**order)    # Adjust the range as needed\n",
    "\n",
    "# Compute the product\n",
    "product = multiplicand * multiplier\n",
    "\n",
    "# Extract digits of the multiplier in reverse order\n",
    "multiplier_digits = [int(d) for d in str(multiplier)][::-1]\n",
    "\n",
    "partial_products = []\n",
    "cumulative_sum = 0\n",
    "cumulative_sums = []\n",
    "\n",
    "for i, digit in enumerate(multiplier_digits):\n",
    "    # partial_product for display\n",
    "    partial_product = multiplicand * digit * 10**i\n",
    "    partial_product_rev = str(partial_product)[::-1]\n",
    "    partial_product_rev = partial_product_rev + '0' * max(0, order+1+i-len(partial_product_rev))\n",
    "    partial_products.append(partial_product_rev)\n",
    "\n",
    "    # Update cumulative sum\n",
    "    cumulative_sum += partial_product\n",
    "\n",
    "    # Reverse cumulative sum for display\n",
    "    cumulative_sum_rev = str(cumulative_sum)[::-1]\n",
    "    cumulative_sum_rev = cumulative_sum_rev + '0' * max(0, order+1+i-len(cumulative_sum_rev))\n",
    "    cumulative_sums.append(cumulative_sum_rev)\n",
    "\n",
    "# Format the chain of thought\n",
    "cot_str = [' '.join(list(partial_products[0]))]\n",
    "for pp, cs in zip(partial_products[1:-1], cumulative_sums[1:-1]):\n",
    "    pp_str = ' '.join([' '.join(list(pp)), '( ' + ' '.join(list(cs)) + ' )'])\n",
    "    cot_str.append(pp_str)\n",
    "cot_str.append(' '.join(list(partial_products[-1])))\n",
    "cot_str = ' + '.join(cot_str)\n",
    "\n",
    "# Format the dataset\n",
    "multiplicand_rev = str(multiplicand)[::-1]\n",
    "multiplier_rev = str(multiplier)[::-1]\n",
    "q_str = ' * '.join([' '.join(list(multiplicand_rev)), ' '.join(list(multiplier_rev))])\n",
    "a_str = ' '.join(list(cumulative_sums[-1]))\n",
    "all_str = f\"{q_str}||{cot_str} #### {a_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 3 6 * 1 6 1||0 3 6 0 + 0 0 8 7 3 ( 0 3 4 8 3 ) + 0 0 0 3 6 0 #### 0 3 4 1 0 1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9 3 8 * 9 1 7||1 5 5 7 + 0 9 3 8 0 ( 1 4 9 5 1 ) + 0 0 3 7 8 5 #### 1 4 2 3 0 6'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'142306'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_sums[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the output as in the example\n",
    "partial_products_str = []\n",
    "for pp in partial_products:\n",
    "    pp_str = ' '.join(list(str(pp)))\n",
    "    partial_products_str.append(pp_str)\n",
    "\n",
    "cumulative_sums_str = []\n",
    "for cs in cumulative_sums[:-1]:  # Exclude the final sum\n",
    "    cs_str = ' '.join(list(str(cs)))\n",
    "    cumulative_sums_str.append(f\"( {cs_str} )\")\n",
    "\n",
    "# Build the final representation\n",
    "lines = []\n",
    "for i in range(len(partial_products_str)):\n",
    "    line = partial_products_str[i]\n",
    "    if i < len(cumulative_sums_str):\n",
    "        line += f\" {cumulative_sums_str[i]}\"\n",
    "    lines.append(line)\n",
    "\n",
    "# Reverse the multiplicand and multiplier digits for display\n",
    "multiplicand_rev = ' '.join(list(str(multiplicand)[::-1]))\n",
    "multiplier_rev = ' '.join(list(str(multiplier)[::-1]))\n",
    "\n",
    "# Output the final representation\n",
    "example = f\"{multiplicand_rev} × {multiplier_rev} = ?\\n\"\n",
    "example += '\\n'.join(lines)\n",
    "example += f\"\\n? {product}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 5 5 7', '0 9 3 8 0', '0 0 3 7 8 5']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_products_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['( 1 5 5 7 )', '( 1 4 9 5 1 )']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_sums_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 3 4 5 * 8 1 9 3\n",
    "||\n",
    "8 4 4 3 4 + 0 1 3 4 5 0 ( 8 5 7 7 9 0 ) + 0 0 9 7 8 8 4 ( 8 5 6 5 8 9 4 ) + 0 0 0 3 9 2 6 1 #### 8 5 6 8 7 2 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Build the final representation\n",
    "lines = []\n",
    "for i in range(len(partial_products_str)):\n",
    "    line = partial_products_str[i]\n",
    "    if i < len(cumulative_sums_str):\n",
    "        line += f\" {cumulative_sums_str[i]}\"\n",
    "    lines.append(line)\n",
    "\n",
    "# Reverse the multiplicand and multiplier digits for display\n",
    "multiplicand_rev = ' '.join(list(str(multiplicand)[::-1]))\n",
    "multiplier_rev = ' '.join(list(str(multiplier)[::-1]))\n",
    "\n",
    "# Output the final representation\n",
    "example = f\"{multiplicand_rev} × {multiplier_rev} = ?\\n\"\n",
    "example += '\\n'.join(lines)\n",
    "example += f\"\\n? {product}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "def reverse_digits(n):\n",
    "    return int(str(n)[::-1])\n",
    "\n",
    "def generate_example():\n",
    "    # Randomly sample two numbers\n",
    "    multiplicand = random.randint(100, 9999)  # Adjust the range as needed\n",
    "    multiplier = random.randint(100, 9999)    # Adjust the range as needed\n",
    "\n",
    "    # Compute the product\n",
    "    product = multiplicand * multiplier\n",
    "\n",
    "    # Extract digits of the multiplier in reverse order\n",
    "    multiplier_digits = [int(d) for d in str(multiplier)][::-1]\n",
    "\n",
    "    partial_products = []\n",
    "    cumulative_sum = 0\n",
    "    cumulative_sums = []\n",
    "\n",
    "    for i, digit in enumerate(multiplier_digits):\n",
    "        # Multiply multiplicand by the current digit\n",
    "        partial_product = multiplicand * digit\n",
    "\n",
    "        # Reverse the digits of the partial product\n",
    "        partial_product_rev = int(str(partial_product)[::-1])\n",
    "\n",
    "        # Shift the partial product by adding zeros to the right\n",
    "        partial_product_shifted = int(str(partial_product_rev) + '0'*i)\n",
    "\n",
    "        # Append to partial products list\n",
    "        partial_products.append(partial_product_shifted)\n",
    "\n",
    "        # Update cumulative sum\n",
    "        cumulative_sum += partial_product_shifted\n",
    "\n",
    "        # Reverse cumulative sum for display\n",
    "        cumulative_sum_rev = int(str(cumulative_sum)[::-1])\n",
    "\n",
    "        # Append cumulative sum to list\n",
    "        cumulative_sums.append(cumulative_sum_rev)\n",
    "\n",
    "    # Format the output as in the example\n",
    "    partial_products_str = []\n",
    "    for pp in partial_products:\n",
    "        pp_str = ' '.join(list(str(pp)))\n",
    "        partial_products_str.append(pp_str)\n",
    "\n",
    "    cumulative_sums_str = []\n",
    "    for cs in cumulative_sums[:-1]:  # Exclude the final sum\n",
    "        cs_str = ' '.join(list(str(cs)))\n",
    "        cumulative_sums_str.append(f\"({cs_str})\")\n",
    "\n",
    "    # Build the final representation\n",
    "    lines = []\n",
    "    for i in range(len(partial_products_str)):\n",
    "        line = partial_products_str[i]\n",
    "        if i < len(cumulative_sums_str):\n",
    "            line += f\" {cumulative_sums_str[i]}\"\n",
    "        lines.append(line)\n",
    "\n",
    "    # Reverse the multiplicand and multiplier digits for display\n",
    "    multiplicand_rev = ' '.join(list(str(multiplicand)[::-1]))\n",
    "    multiplier_rev = ' '.join(list(str(multiplier)[::-1]))\n",
    "\n",
    "    # Output the final representation\n",
    "    example = f\"{multiplicand_rev} × {multiplier_rev} = ?\\n\"\n",
    "    example += '\\n'.join(lines)\n",
    "    example += f\"\\n? {product}\"\n",
    "\n",
    "    return example\n",
    "\n",
    "# Generate 808,000 training examples and 1,000 validation examples\n",
    "num_training = 808000\n",
    "num_validation = 1000\n",
    "\n",
    "training_set = set()\n",
    "validation_set = set()\n",
    "\n",
    "# Generate training data\n",
    "while len(training_set) < num_training:\n",
    "    example = generate_example()\n",
    "    equation = example.split('\\n')[0]  # Use the first line as a unique identifier\n",
    "    if equation not in training_set:\n",
    "        training_set.add(example)\n",
    "\n",
    "# Generate validation data\n",
    "while len(validation_set) < num_validation:\n",
    "    example = generate_example()\n",
    "    equation = example.split('\\n')[0]\n",
    "    if equation not in training_set and equation not in validation_set:\n",
    "        validation_set.add(example)\n",
    "\n",
    "# Write training data to a file\n",
    "with open('training_data.txt', 'w') as f:\n",
    "    for example in training_set:\n",
    "        f.write(example + '\\n\\n')\n",
    "\n",
    "# Write validation data to a file\n",
    "with open('validation_data.txt', 'w') as f:\n",
    "    for example in validation_set:\n",
    "        f.write(example + '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/yonghyun/anaconda3/envs/ttc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import re\n",
    "import torch\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "import argparse\n",
    "import os\n",
    "import inspect\n",
    "import tqdm\n",
    "import logging\n",
    "import random\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.data import CoTDataset, CoTDataCollator, extract_answer\n",
    "from src.models.emulator import Emulator\n",
    "from src.models.student import Student\n",
    "from src.utils import get_sep_position\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--test_path', type=str, required=True)\n",
    "# parser.add_argument('--batch_size', type=int, default=1)\n",
    "# parser.add_argument('--max_new_tokens', type=int, default=128)\n",
    "# parser.add_argument('--student_path', type=str, required=True)\n",
    "# parser.add_argument('--emulator_path', type=str, required=True)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "bsz = 1\n",
    "student_path = \"models/4_by_4_mult/gpt2/student\"\n",
    "emulator_path = \"models/4_by_4_mult/gpt2/emulator\"\n",
    "\n",
    "class Args:\n",
    "    test_path = \"data/4_by_4_mult/test_bigbench.txt\"\n",
    "    batch_size = bsz\n",
    "    max_new_tokens = 128\n",
    "    student_path = student_path\n",
    "    emulator_path = emulator_path\n",
    "\n",
    "args = Args()\n",
    "print(args)\n",
    "\n",
    "dtype = 'float32'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ctx = torch.amp.autocast(device_type='cuda', dtype=ptdtype)\n",
    "print(ptdtype, dtype, device)\n",
    "\n",
    "# Load Models\n",
    "emulator = Emulator.from_pretrained(args.emulator_path).to(device).to(ptdtype)\n",
    "student = Student.from_pretrained(args.student_path).to(device).to(ptdtype)\n",
    "emulator.eval()\n",
    "student.eval()\n",
    "\n",
    "# Load data\n",
    "tokenizer = emulator.tokenizer\n",
    "collate_fn = CoTDataCollator(tokenizer)\n",
    "test_dataset = CoTDataset(tokenizer, args.test_path, 1024)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "accuracy, throughput  = evaluate(test_dataloader, tokenizer, ctx, emulator, student, args.max_new_tokens)\n",
    "print(f\"Test Accuracy: {accuracy}. Throughput: {throughput}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(dataloader, tokenizer, ctx, emulator, student, max_new_tokens):\n",
    "    total_time = 0\n",
    "    total_instances = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        input_ids_all = batch['input_ids_nocot'].to(device)\n",
    "        # Remove answer part\n",
    "        sep_positions = get_sep_position(input_ids_all, tokenizer.eos_token_id)\n",
    "        input_ids = input_ids_all[:, :sep_positions.max()+1]\n",
    "        start_time = time.time()\n",
    "        with ctx:\n",
    "            emulated_teacher_states = emulator(input_ids)\n",
    "\n",
    "            # Generate from student\n",
    "            beam_output = student.generate(\n",
    "                input_ids=input_ids,\n",
    "                teacher_states=emulated_teacher_states,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "\n",
    "        # Evaluate\n",
    "        #import pdb; pdb.set_trace()\n",
    "        for i, (input_ids_all_i, beam_output_i) in enumerate(zip(input_ids_all, beam_output)):\n",
    "            #sep_position = input_ids_single.tolist().index(tokenizer.eos_token_id)\n",
    "            sep_position = sep_positions[i].item()\n",
    "            tgt = input_ids_all_i[sep_position+1:]\n",
    "            tgt_text = tokenizer.decode(tgt, skip_special_tokens=True)\n",
    "            ans = extract_answer(tgt_text)\n",
    "            pred_text = tokenizer.decode(beam_output_i[0][sep_position+1:], skip_special_tokens=True)\n",
    "            pred_ans = extract_answer(pred_text)\n",
    "            #import pdb; pdb.set_trace()\n",
    "            total_instances += 1\n",
    "            if ans == pred_ans:\n",
    "                total_correct += 1\n",
    "        end_time = time.time()\n",
    "        total_time += end_time - start_time\n",
    "\n",
    "    #print (total_time, total_instances, total_instances / total_time)\n",
    "    throughput = total_instances / total_time\n",
    "    accuracy = total_correct / total_instances\n",
    "    return accuracy, throughput\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--test_path', type=str, required=True)\n",
    "    parser.add_argument('--batch_size', type=int, default=1)\n",
    "    parser.add_argument('--max_new_tokens', type=int, default=128)\n",
    "    parser.add_argument('--student_path', type=str, required=True)\n",
    "    parser.add_argument('--emulator_path', type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print (args)\n",
    "    \n",
    "    dtype = 'float32'\n",
    "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ctx = torch.amp.autocast(device_type='cuda', dtype=ptdtype)\n",
    "    print (ptdtype, dtype, device)\n",
    "\n",
    "\n",
    "    # Load Models\n",
    "    emulator = Emulator.from_pretrained(args.emulator_path).to(device).to(ptdtype)\n",
    "    student = Student.from_pretrained(args.student_path).to(device).to(ptdtype)\n",
    "    emulator.eval()\n",
    "    student.eval()\n",
    "\n",
    "    # Load data\n",
    "    tokenizer = emulator.tokenizer\n",
    "    collate_fn = CoTDataCollator(tokenizer)\n",
    "    test_dataset = CoTDataset(tokenizer, args.test_path, 1024)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    accuracy, throughput  = evaluate(test_dataloader, tokenizer, ctx, emulator, student, args.max_new_tokens)\n",
    "    print (f\"Test Accuracy: {accuracy}. Throughput: {throughput}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
