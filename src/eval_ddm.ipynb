{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd1/yonghyun/anaconda3/envs/ttc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import argparse\n",
    "import os\n",
    "import tqdm\n",
    "import inspect\n",
    "import logging\n",
    "\n",
    "from models.teacher import Teacher\n",
    "from models.configuration_teacher import TeacherConfig\n",
    "from data import CoTDataset, CoTDataCollator, extract_answer\n",
    "\n",
    "from utils import get_sep_position\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_pretrained_model(args):\n",
    "    if args.base_model == \"sedd\":\n",
    "        # load model\n",
    "        from ddms.sedd import SEDD\n",
    "        model = SEDD.from_pretrained(\"louaaron/sedd-small\")\n",
    "\n",
    "        # load config\n",
    "        args.num_vocabs = model.config.tokens\n",
    "        args.length = model.config.model.length\n",
    "        args.noise_schedule = model.config.noise.type\n",
    "        args.graph = 'absorb'\n",
    "    \n",
    "    if args.base_model == \"mdlm\":\n",
    "        model = AutoModelForMaskedLM.from_pretrained(\"kuleshov-group/mdlm-owt\", trust_remote_code=True)\n",
    "        \n",
    "        # load config\n",
    "        args.num_vocabs = model.config.vocab_size - 1\n",
    "        args.length = model.config.model_length\n",
    "        args.noise_schedule = 'loglinear'\n",
    "        args.graph = 'absorb'\n",
    "    \n",
    "    return model, args\n",
    "\n",
    "def load_diffusion_scheduler(args):\n",
    "    if args.base_model == \"sedd\":\n",
    "        pass\n",
    "    if args.base_model == \"mdlm\":\n",
    "        from ddms import mdlm\n",
    "        if args.scheduler_name == \"euler\":\n",
    "            scheduler = mdlm.EulerScheduler(args)\n",
    "        if args.scheduler_name == \"maskgit\":\n",
    "            scheduler = mdlm.MaskGITScheduler(args)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    test_path = \"../data/gsm8k/test.txt\"\n",
    "    model_path = \"../train_models/gsm8k/mdlm/teacher/checkpoint_0/model.safetensors\"\n",
    "    max_new_tokens = 1024\n",
    "    batch_size = 1\n",
    "    base_model = 'mdlm'\n",
    "    scheduler_name = 'maskgit'\n",
    "    num_inf = 16\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 float32 cuda 0\n",
      "Creating features from dataset file at ../data/gsm8k/test.txt\n",
      "tgt_avg:  27.708870356330554\n",
      "src_avg:  57.5352539802881\n",
      "ratios:  2.076420050344752\n",
      "tgt_avg:  6.0962850644427595\n",
      "src_avg:  57.5352539802881\n",
      "ratios:  9.437756497948017\n",
      " Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? <|endoftext|> <<16-3-4=9>> <<9*2=18>> <|endoftext|> #### 18 <|endoftext|>\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9959, 1433, 12, 18, 12, 19, 28, 24, 4211, 9959, 24, 9, 17, 28, 1507, 4211, 220, 50256]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1303, 21017, 1248, 220, 50256]\n",
      "[28111, 447, 247, 82, 39694, 3830, 1467, 9653, 583, 1110, 13, 1375, 25365, 1115, 329, 12607, 790, 3329, 290, 275, 1124, 27563, 1040, 329, 607, 2460, 790, 1110, 351, 1440, 13, 1375, 16015, 262, 17675, 379, 262, 9818, 6, 1910, 4445, 329, 720, 17, 583, 4713, 22045, 5935, 13, 1374, 881, 287, 5054, 857, 673, 787, 790, 1110, 379, 262, 9818, 6, 1910, 30, 220, 50256, 1303, 21017, 1248, 220, 50256]\n",
      " Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? <|endoftext|> #### 18 <|endoftext|>\n",
      "[28111, 447, 247, 82, 39694, 3830, 1467, 9653, 583, 1110, 13, 1375, 25365, 1115, 329, 12607, 790, 3329, 290, 275, 1124, 27563, 1040, 329, 607, 2460, 790, 1110, 351, 1440, 13, 1375, 16015, 262, 17675, 379, 262, 9818, 6, 1910, 4445, 329, 720, 17, 583, 4713, 22045, 5935, 13, 1374, 881, 287, 5054, 857, 673, 787, 790, 1110, 379, 262, 9818, 6, 1910, 30, 220]\n",
      "[50256, 1303, 21017, 1248, 220, 50256]\n"
     ]
    }
   ],
   "source": [
    "dtype = 'float32'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ctx = torch.amp.autocast(device_type='cuda', dtype=ptdtype)\n",
    "print(ptdtype, dtype, device, torch.cuda.current_device()\n",
    ")\n",
    "\n",
    "# Load finetuned model \n",
    "teacher, args = load_pretrained_model(args)\n",
    "teacher.load_state_dict(load_file(args.model_path))\n",
    "scheduler = load_diffusion_scheduler(args)\n",
    "teacher = teacher.to(device, ptdtype)\n",
    "\n",
    "# Load data\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "collate_fn = CoTDataCollator(tokenizer)\n",
    "test_dataset = CoTDataset(tokenizer, args.test_path, 1024)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1319 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    dataloader, tokenizer, ctx, teacher, scheduler, num_inf, loss_fn\n",
    ") = (\n",
    "    test_dataloader, tokenizer, ctx, teacher, scheduler, args.num_inf, None\n",
    ")\n",
    "teacher.eval()\n",
    "total_instances = 0\n",
    "total_tokens = 0\n",
    "total_correct = 0\n",
    "total_correct_tokens = 0\n",
    "total_loss = 0\n",
    "for batch in tqdm.tqdm(dataloader):\n",
    "    input_ids_all = batch['input_ids_all'].to(device)\n",
    "    labels = batch['labels_all'].to(device)\n",
    "    break\n",
    "\n",
    "# Remove answer part\n",
    "sep_positions = get_sep_position(input_ids_all, tokenizer.eos_token_id)\n",
    "input_ids = input_ids_all\n",
    "input_ids[:, sep_positions.max():] = scheduler.mask_idx\n",
    "batch_size = input_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "# gen_output = scheduler.euler_sample(\n",
    "#     teacher, xt=input_ids, \n",
    "#     t=1, s=1e-5, num_inference_steps=num_inf\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yonghyun/.cache/huggingface/modules/transformers_modules/kuleshov-group/mdlm-owt/9e6829bb908d241a074146e4c5c095238bb5e316/modeling_mdlm.py:397: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
      "/home/yonghyun/.cache/huggingface/modules/transformers_modules/kuleshov-group/mdlm-owt/9e6829bb908d241a074146e4c5c095238bb5e316/modeling_mdlm.py:147: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/yonghyun/.cache/huggingface/modules/transformers_modules/kuleshov-group/mdlm-owt/9e6829bb908d241a074146e4c5c095238bb5e316/modeling_mdlm.py:285: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    }
   ],
   "source": [
    "model=teacher\n",
    "xt=input_ids\n",
    "num_inference_steps=num_inf\n",
    "\n",
    "length = (xt == scheduler.mask_idx).sum(dim=1)\n",
    "\n",
    "eps = 1e-3\n",
    "t = torch.linspace(1, eps, num_inference_steps + 1, device=xt.device)\n",
    "k = (1 - (-scheduler.sigma_bar(t)).exp()) * length\n",
    "k = k.long()\n",
    "k[-1] = 0\n",
    "\n",
    "for i in range(num_inference_steps):\n",
    "    dk = k[i] - k[i+1]\n",
    "    sigma_bar_t = scheduler.sigma_bar(k[None, i])\n",
    "    output = model(xt, torch.zeros_like(sigma_bar_t))\n",
    "    break\n",
    "    output = scheduler.step(output, xt, dk)\n",
    "    xt = output.xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m conf[x0 \u001b[38;5;241m!=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mmask_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     20\u001b[0m conf_v, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(conf, step_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m mask \u001b[38;5;241m=\u001b[39m (conf \u001b[38;5;241m-\u001b[39m conf_v[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :])\u001b[38;5;241m.\u001b[39mto(xt\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     23\u001b[0m xs \u001b[38;5;241m=\u001b[39m mask \u001b[38;5;241m*\u001b[39m xt \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask) \u001b[38;5;241m*\u001b[39m x0\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step_size = dk\n",
    "\n",
    "def sample_categorical(categorical_probs, eps=1e-6, generator=None):\n",
    "    '''use gumbel-max trick, but given probability'''\n",
    "    if generator is None:\n",
    "        gumbel_noise = torch.rand_like(categorical_probs)\n",
    "    else:\n",
    "        gumbel_noise = torch.rand(categorical_probs.shape, generator=generator, device=generator.device).to(categorical_probs)\n",
    "    gumbel_noise = (eps - torch.log(eps + (1 - eps) * gumbel_noise))\n",
    "    return torch.argmax(categorical_probs / gumbel_noise, dim=-1), gumbel_noise\n",
    "\n",
    "# generate x0 ~ p_x0\n",
    "logits = scheduler.output_to_logits(output, xt)\n",
    "p_x0 = logits.exp()\n",
    "x0, noise = sample_categorical(p_x0)\n",
    "\n",
    "# mask x0 w.r.t confidence \n",
    "conf = torch.gather(p_x0, -1, x0[..., None])\n",
    "conf[x0 != scheduler.mask_idx] = -torch.inf\n",
    "conf_v, _ = torch.topk(conf, step_size, dim=1)\n",
    "assert False\n",
    "mask = (conf - conf_v[None, None, :]).to(xt.dtype)\n",
    "xs = mask * xt + (1 - mask) * x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 153, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yonghyun/.cache/huggingface/modules/transformers_modules/kuleshov-group/mdlm-owt/9e6829bb908d241a074146e4c5c095238bb5e316/modeling_mdlm.py:397: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
      "/home/yonghyun/.cache/huggingface/modules/transformers_modules/kuleshov-group/mdlm-owt/9e6829bb908d241a074146e4c5c095238bb5e316/modeling_mdlm.py:147: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/yonghyun/.cache/huggingface/modules/transformers_modules/kuleshov-group/mdlm-owt/9e6829bb908d241a074146e4c5c095238bb5e316/modeling_mdlm.py:285: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    }
   ],
   "source": [
    "model=teacher\n",
    "xt=input_ids\n",
    "t=1. * torch.ones(batch_size, device=xt.device)\n",
    "s=1e-5 * torch.ones(batch_size, device=xt.device)\n",
    "num_inference_steps=num_inf\n",
    "\n",
    "timesteps = torch.linspace(1, scheduler.eps, num_inference_steps+1, device=xt.device)\n",
    "timesteps = (t[:, None] - s[:, None]) * timesteps[None, :] + s[:, None]\n",
    "for i in range(num_inference_steps):\n",
    "    dt = timesteps[:, i] - timesteps[:, i+1]\n",
    "    curr_t = timesteps[:, i]\n",
    "\n",
    "    sigma_bar_t = scheduler.sigma_bar(curr_t)\n",
    "    output = model(xt, torch.zeros_like(sigma_bar_t))\n",
    "    output = scheduler.step(output, xt, curr_t, dt)\n",
    "    xt = output.xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Mrs. Tatiana owns a grocery store that sells different fruits and vegetables, which includes carrots. The price of carrots in the grocery store increases by 5% of the original price every year. What would be the price of carrots after three years if it was $120 initially? (Round to the nearest integer)  <<106 #### 633 587=26.26*=6=46>>126+46= 8.26 <|endoftext|> # 26 <|endoftext|>'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(xt[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Mrs. Tatiana owns a grocery store that sells different fruits and vegetables, which includes carrots. The price of carrots in the grocery store increases by 5% of the original price every year. What would be the price of carrots after three years if it was $120 initially? (Round to the nearest integer) '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gen_output = scheduler.generate(\n",
    "#     input_ids=input_ids,\n",
    "#     num_inf=num_inf,\n",
    "# )\n",
    "# Evaluate\n",
    "#import pdb; pdb.set_trace()\n",
    "for i, (input_ids_all_i, gen_output_i) in enumerate(zip(input_ids_all, gen_output)):\n",
    "    sep_position = sep_positions[i].item()\n",
    "    tgt = input_ids_all_i[sep_position+1:]\n",
    "    tgt_text = tokenizer.decode(tgt, skip_special_tokens=True)\n",
    "    ans = extract_answer(tgt_text)\n",
    "    pred_text = tokenizer.decode(gen_output_i[0][sep_position+1:], skip_special_tokens=True)\n",
    "    pred_ans = extract_answer(pred_text)\n",
    "    if ans == pred_ans:\n",
    "        total_correct += 1\n",
    "    if i == 0:\n",
    "        print(f'Input: {tokenizer.decode(input_ids_all_i[:sep_position], skip_special_tokens=True)}')\n",
    "        print(f'Target: {tgt_text}')\n",
    "        print(f'Predicted: {pred_text}')\n",
    "        print('')\n",
    "accuracy = total_correct / total_instances\n",
    "token_accuracy = total_correct_tokens / total_tokens\n",
    "loss = total_loss / total_tokens\n",
    "ppl = math.exp(loss)\n",
    "return accuracy, token_accuracy, ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    input_ids_all = batch['input_ids_all'].to(device)\n",
    "    labels = batch['labels_all'].to(device)\n",
    "    # Remove answer part\n",
    "    sep_positions = get_sep_position(input_ids_all, tokenizer.eos_token_id)\n",
    "    input_ids = input_ids_all\n",
    "    input_ids[:, :sep_positions.max()+1] = scheduler.mask_idx\n",
    "    batch_size = input_ids.shape[0]\n",
    "    if loss_fn:\n",
    "        with ctx:\n",
    "            outputs = loss_fn(input_ids=input_ids_all, labels=labels)\n",
    "        total_loss += outputs.total_loss.item()\n",
    "        total_correct_tokens += outputs.total_correct.item()\n",
    "        total_tokens += outputs.total_tokens\n",
    "        total_instances += batch_size\n",
    "\n",
    "    # Generate\n",
    "    gen_output = scheduler.euler_sample(\n",
    "        teacher, xt=input_ids, \n",
    "        t=1, s=1e-5, num_inference_steps=num_inf\n",
    "    )\n",
    "    # gen_output = scheduler.generate(\n",
    "    #     input_ids=input_ids,\n",
    "    #     num_inf=num_inf,\n",
    "    # )\n",
    "    # Evaluate\n",
    "    #import pdb; pdb.set_trace()\n",
    "    for i, (input_ids_all_i, gen_output_i) in enumerate(zip(input_ids_all, gen_output)):\n",
    "        sep_position = sep_positions[i].item()\n",
    "        tgt = input_ids_all_i[sep_position+1:]\n",
    "        tgt_text = tokenizer.decode(tgt, skip_special_tokens=True)\n",
    "        ans = extract_answer(tgt_text)\n",
    "        pred_text = tokenizer.decode(gen_output_i[0][sep_position+1:], skip_special_tokens=True)\n",
    "        pred_ans = extract_answer(pred_text)\n",
    "        if ans == pred_ans:\n",
    "            total_correct += 1\n",
    "        if i == 0:\n",
    "            print(f'Input: {tokenizer.decode(input_ids_all_i[:sep_position], skip_special_tokens=True)}')\n",
    "            print(f'Target: {tgt_text}')\n",
    "            print(f'Predicted: {pred_text}')\n",
    "            print('')\n",
    "accuracy = total_correct / total_instances\n",
    "token_accuracy = total_correct_tokens / total_tokens\n",
    "loss = total_loss / total_tokens\n",
    "ppl = math.exp(loss)\n",
    "return accuracy, token_accuracy, ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = 'float32'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ctx = torch.amp.autocast(device_type='cuda', dtype=ptdtype)\n",
    "print (ptdtype, dtype, device)\n",
    "\n",
    "# Create Teacher \n",
    "teacher, args = load_pretrained_model(args)\n",
    "scheduler = load_diffusion_scheduler(args)\n",
    "\n",
    "# Load data\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "collate_fn = CoTDataCollator(tokenizer)\n",
    "train_dataset = CoTDataset(tokenizer, args.train_path, 1024)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataset = CoTDataset(tokenizer, args.val_path, 1024)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12020 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm.tqdm(train_dataloader):\n",
    "    input_ids_all = batch['input_ids_all'].to(device)\n",
    "    labels = batch['labels_all'].to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.rand(4,4)\n",
    "xt = torch.zeros(4,4)\n",
    "cond = x0 > 0.5\n",
    "xt[cond] = x0[cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A lion needs to gain 500 pounds for the winter. In the summer, it feasts on zebras and during autumn, it hunts gazelles and buffalos. It gained half its weight from zebras during summer and during autumn, it gained a quarter of that amount from gazelles. Buffalos made up the rest of its diet. How many pounds did it gain eating buffalos? <|endoftext|> \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(batch['input_ids_only'][1].tolist()))\n",
    "print(tokenizer.decode([220]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3,4,5,6)\n",
    "x[x > 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1002, 27775, 13267,  ..., 50256, 50256, 50256],\n",
       "        [  317, 18744,  2476,  ...,   220, 50256,   220],\n",
       "        [ 8114,   468,   642,  ..., 50256, 50256, 50256],\n",
       "        ...,\n",
       "        [ 3362,  6593, 19132,  ..., 50256, 50256, 50256],\n",
       "        [25737,  6134,  3126,  ..., 50256, 50256, 50256],\n",
       "        [ 1629,   257,  3807,  ..., 50256, 50256, 50256]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If Kanye decides to jog around the park for 3 hours and a bottle of water is 500ml and costs $0.5. He drinks 1 bottle after each hour to stay hydrated, how much does he spend on water in total? <|endoftext|> <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      " A lion needs to gain 500 pounds for the winter. In the summer, it feasts on zebras and during autumn, it hunts gazelles and buffalos. It gained half its weight from zebras during summer and during autumn, it gained a quarter of that amount from gazelles. Buffalos made up the rest of its diet. How many pounds did it gain eating buffalos? <|endoftext|> \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(batch['input_ids_only'][0].tolist()))\n",
    "print(tokenizer.decode(batch['input_ids_only'][1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['input_ids_only'].shape)\n",
    "print(batch['input_ids_cot'].shape)\n",
    "print(batch['input_ids_nocot'].shape)\n",
    "print(batch['input_ids_all'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids_only'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids_all'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm.tqdm(val_dataloader):\n",
    "    input_ids_all = batch['input_ids_all'].to(device)\n",
    "    labels = batch['labels_all'].to(device)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
